{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M27qF7CTrBqc"
      },
      "source": [
        "# PROBLEM STATEMENT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNl52nl3qiyL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "* Dataset includes house sale prices for King County in USA. \n",
        "* Homes that are sold in the time period: May, 2014 and May, 2015.\n",
        "* Columns:\n",
        "\n",
        "> 1. ida: notation for a house\n",
        "1. date: Date house was sold\n",
        "2. price: Price is prediction target\n",
        "3. bedrooms: Number of Bedrooms/House\n",
        "4. bathrooms: Number of bathrooms/House\n",
        "5. sqft_living: square footage of the home\n",
        "6. sqft_lot: square footage of the lot\n",
        "7. floors: Total floors (levels) in house\n",
        "8. waterfront: House which has a view to a waterfront\n",
        "9.  view: Has been viewed\n",
        "10. condition: How good the condition is ( Overall )\n",
        "11. grade: overall grade given to the housing unit, based on King County grading system\n",
        "12. sqft_abovesquare: footage of house apart from basement\n",
        "13. sqft_basement: square footage of the basement\n",
        "14. yr_built: Built Year\n",
        "15. yr_renovated: Year when house was renovated\n",
        "16. zipcode: zip\n",
        "17. lat: Latitude coordinate\n",
        "18. long: Longitude coordinate\n",
        "19. sqft_living15: Living room area in 2015(implies-- some renovations) \n",
        "20. sqft_lot15: lotSize area in 2015(implies-- some renovations)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKmFmyaGunc7"
      },
      "source": [
        "STEP 0: IMPORT LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "id": "S0Cx3743urFY",
        "outputId": "ed36d89b-83c3-4e4d-a42a-4c184c4e4ba3"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow-gpu==2.0.0.alpha0\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import Ridge\n",
        "from  datetime import datetime \n",
        "from datetime import timedelta\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from sklearn.feature_selection import SelectKBest,f_regression\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAE6Icc0uylP"
      },
      "source": [
        "STEP 1: IMPORT DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "5P_ciLXawMZg",
        "outputId": "97af15d0-51fb-478b-8169-45a17fefbd92"
      },
      "outputs": [],
      "source": [
        "# You will need to mount your drive using the following commands:\n",
        "# For more information regarding mounting, please check this out: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjIiJdM4u1IE"
      },
      "outputs": [],
      "source": [
        "# You have to include the full link to the csv file containing your dataset\n",
        "df = pd.read_csv('kc_house_data.csv', encoding = 'ISO-8859-1')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q4_wPDKCu5Uc",
        "outputId": "cb83b54f-d854-4e8d-b502-d87e352c2eee"
      },
      "outputs": [],
      "source": [
        "df.drop(['id','date'], axis=1, inplace=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().any().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Sorted heatmap\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(df.corr().sort_values(by='price').T[::-1],cmap='coolwarm',annot=True, fmt=\".2f\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#UNIVARIATE SELECTION\n",
        "# Feature Extraction with Univariate Statistical Tests (f_regression)\n",
        "\n",
        "# load data\n",
        "X = df.drop('price',axis=1)\n",
        "y = df['price']\n",
        "names=pd.DataFrame(X.columns)\n",
        "\n",
        "model = SelectKBest(score_func=f_regression, k=4)\n",
        "results = model.fit(X, y)\n",
        "\n",
        "print (results.scores_)\n",
        "\n",
        "results_df=pd.DataFrame(results.scores_)\n",
        "#Concat and name columns\n",
        "scored=pd.concat([names,results_df], axis=1)\n",
        "scored.columns = [\"Feature\", \"Score\"]\n",
        "scored.sort_values(by=['Score'],ascending=False)\n",
        "final_columns = scored[scored.Score >1]\n",
        "final_columns.sort_values(by=['Score'],ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_csv('final_df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### STEP #3: CREATE TESTING AND TRAINING DATASET/DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_final = pd.read_csv('final_df.csv')\n",
        "df_final.drop('Unnamed: 0',axis=1, inplace=True)\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_final.drop('price', axis=1)\n",
        "y = df_final['price']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 5)\n",
        "# scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(neurons):\n",
        "\t# create model\n",
        "    # Init the model\n",
        "    reg_model = tf.keras.models.Sequential()\n",
        "    # First layer inputs\n",
        "    reg_model.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
        "    # hidden layers number 1\n",
        "    #reg_model.add(tf.keras.layers.BatchNormalization())\n",
        "    reg_model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
        "    #reg_model.add(tf.keras.layers.Dropout(0.2))          \n",
        "    # hidden layers number 2\n",
        "    #reg_model.add(tf.keras.layers.BatchNormalization())  \n",
        "    reg_model.add(tf.keras.layers.Dense(neurons//2 , activation='relu'))\n",
        "    #reg_model.add(tf.keras.layers.Dropout(0.2))\n",
        "    # hidden layers number 3\n",
        "    #reg_model.add(tf.keras.layers.BatchNormalization())\n",
        "    reg_model.add(tf.keras.layers.Dense(neurons//4 , activation='relu'))\n",
        "    # Last layer\n",
        "    reg_model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    # print summary to undertstand your neural network flow\n",
        "    reg_model.summary()\n",
        "\n",
        "    return reg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model_functional(layer_sizes):\n",
        "\t# create model\n",
        "    # Init the model\n",
        "    reg_model = tf.keras.models.Sequential()\n",
        "    # First layer inputs\n",
        "    reg_model.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
        "    # hidden layers number 1\n",
        "    \n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      reg_model.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      reg_model.add(tf.keras.layers.Dense(layer_size))\n",
        "\n",
        "    # print summary to undertstand your neural network flow\n",
        "    reg_model.summary()\n",
        "\n",
        "    return reg_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_model_functional([128,64,32,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAINING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 7\n",
        "tf.random.set_seed(seed)\n",
        "# create model\n",
        "model = KerasRegressor(model=create_model, \n",
        "                        loss=\"mean_squared_error\", \n",
        "                        optimizer=\"Adam\", \n",
        "                        metrics=['mae','mse'],\n",
        "                        verbose=1)\n",
        "# define the grid search parameters\n",
        "neurons = [64]\n",
        "learn_rate = [0.001]\n",
        "batch_size = [50]\n",
        "epochs = [50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = dict(optimizer__learning_rate=learn_rate, batch_size=batch_size, epochs= epochs, model__neurons=neurons)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n",
        "grid_result = grid.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_result.best_params_['model__neurons']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Optimizer = tf.keras.optimizers.Adam(learning_rate=grid_result.best_params_['optimizer__learning_rate'])\n",
        "model = create_model(grid_result.best_params_['model__neurons'])\n",
        "model.compile(Optimizer, \n",
        "              loss='mean_squared_error', \n",
        "              metrics=['mae','mse'])\n",
        "epochs_hist = model.fit(X_train,\n",
        "                        y_train,\n",
        "                        validation_data=(X_test , y_test),\n",
        "                        batch_size=grid_result.best_params_['batch_size'], \n",
        "                        epochs=grid_result.best_params_['epochs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using Kfold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the model\n",
        "def larger_model():\n",
        "\t# create model\n",
        "\tmodel = tf.keras.models.Sequential()\n",
        "\tmodel.add(tf.keras.Input(shape=(X_train.shape[1],)))\n",
        "    # hidden layers number 1\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization())\n",
        "\tmodel.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\tmodel.add(tf.keras.layers.Dropout(0.2))          \n",
        "    # hidden layers number 2\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization())\n",
        "\tmodel.add(tf.keras.layers.Dense(64 , activation='relu'))\n",
        "\tmodel.add(tf.keras.layers.Dropout(0.2))\n",
        "    # hidden layers number 3\n",
        "\tmodel.add(tf.keras.layers.BatchNormalization())\n",
        "\tmodel.add(tf.keras.layers.Dense(16 , activation='relu'))\n",
        "    # Last layer\n",
        "\tmodel.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "    # print summary to undertstand your neural network flow\n",
        "\tmodel.summary()\n",
        "\tmodel.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\treturn model\n",
        "\n",
        "# evaluate model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasRegressor(model=larger_model, epochs=50, batch_size=5, verbose=1)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = KFold(n_splits=3)\n",
        "results = cross_val_score(pipeline, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
        "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "r2_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "4. Predict House Prices using ANNs (Regression Task) - Solutions.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "713b5dbaf9691cfa9873b61b3538f4ed3447b354f31f667d1ac5f1f5907b30d9"
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "817cb22576e634a3081313ec8360d8b577ff16da2473220360f42df4e1ca6516"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
